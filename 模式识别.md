# 2 聚类分析

## 2.2 相似性测度和聚类准则

### 相似性测度

#### 欧氏距离

> 设$X_1$、$X_2$为两个n维模式样本，$X_1=[x_{11},x_{12},...,x_{1n}]^T, X_2=[x_{21},x_{22},...,x_{2n}]^T$ 
>
> 欧氏距离定义为：
>
> $D(X_1,X_2) = ||X_1-X_2||=\sqrt{(X_1-X_2)^T(X_1-X_2)}=\sqrt{(x_{11}-x_{21})^2+...+(x_{1n}-x_{2n})^2}$
>
> 距离越小，越相似。

注意：

- 各特征维上应当是相同的物理量（x11和x21应该是一样的物理量）
- 注意同类物理量的量纲应该一样



#### 马氏距离

> $D^2=(X-M)^TC^{-1}(X-M)$
>
> X：模式向量；M：均值向量；
>
> C：该类模式总体的协方差矩阵



#### 明氏距离

> $D_m(X_i,X_j)=[\sum^n_{k=1}|x_{ik}-x_{jk}|^m]^{1/m}$
>
> 也c称为m-范数。

- 当m=2时，明氏距离为欧氏距离

- 当m=1时，称为“街坊”距离：

  $D_1(X_i,X_j) = \sum^n_{k=1}|x_{ik}-x_{jk}|$

![1](模式识别.assets/1.PNG)

#### 汉明距离

> 设Xi和Xj为n维二值模式（分量取值1或-1）样本向量，则
>
> $D_h(X_i,X_j)=\frac {1}{2}(n-\sum^n_{k=1}x_{ik}·x_{jk})$

- 该式中的**求和式**，表达的是两个二值向量之间，同值分量（即值相等的分量）与不同值分量数之差，该差值越大，则两个向量越相似。因为取了负，所以$n-\sum^n_{k=1}x_{ik}-x_{jk}$越大，则两个向量越不相似。
- 当$n-\sum^n_{k=1}x_{ik}-x_{jk}$取值为：
  - 0：表示两模式向量的各个分量相同
  - n：表示两模式向量的各个分量都不同
  - n/2：表示两模式分量中，取值相同的分量数比取值不同的分量数多n/2

#### 角度相似性函数

> $S(X_i,X_j)=\frac{X^T_iX_j}{||X_i||·||X_j||}$
>
> 即两向量之间夹角的余弦。



#### Tanimoto测度

> 用于0-1型二值特征情况，定义：
>
> $S(X_i,X_j)=\frac{X_i^TX_j}{X^T_iX_i+X^T_jX_j-X^T_iX_j}=\frac{X_i,X_j之间具有某特征的数量}{X_i,X_j中有某种特征的分量总数}$



### 聚类准则

- 确定聚类准则的两种方式：

  - 阈值准则：根据规定的距离阈值进行判断
  - 函数准则：利用聚类准则函数进行判断

- 聚类准则函数：

  - 常用指标：误差平方之和：

    $J=\sum^c_{j=1}\sum_{X\in S_j}||X-M_j||^2$

    - c为聚类类别的数目
    - $M_j=\frac{1}{N_j}\sum_{x\in S_j}X$是属于$S_j$集的样本的均值向量，$N_j$为模式类$S_j$中样本数目

  适用于各类样本密集且数目相差不多，而不同类间的样本又明显分开的情况。

## 2.3 基于距离阈值的聚类算法

### 近邻聚类法

- 算法描述
  1. 任取样本Xi作为第一个聚类中心的初始值Z1
  2. 计算样本X2到Z1的欧氏距离D，若D>T，定义一新的聚类分析Z2=X2，否则X2属于以Zi为中心的聚类
  3. 设已有聚类中心Z1, Z2，计算D31和D32，若D31>T且D32>T，则建立第三个聚类中心
  4. 依次类推

### 最大最小距离算法

- 算法描述
  1. 选任意一模式样本作为第一聚类中心Z1
  2. 选择**离Z1距离最远的样本**作为第二聚类中心Z2
  3. 逐个计算各模式样本Xi与已确定的所有聚类中心Zi之间的距离，并选出其中最小的距离。
  4. 在**所有最小距离中选出最大距离**，如该最大值达到$||Z_1-Z_2||$的一定分数比值以上，则相应的样本点取为心得聚类中心
  5. 重复，直到没有新的聚类中心出现为止。
  6. 将剩余样本按最近距离划分到相应的聚类中心对应的类中

### 层次聚类法（系统聚类法、谱系聚类法）

- 算法描述

  1. N个初始模式样本自成一类，即建立N类：

     $G_1(0),G_2(0),...,G_N(0))$

     计算各类之间的距离，得一N*N维距离矩阵D(0)，0表示初始状态

  2. 假设已求的距离矩阵D(n)，找出D(n)中的最小元素，将其对应的两类合并为一类，由此建立新的分类：

     $G_1(n+1),G_2(n+1),...$

  3. 再次计算：经过合并后，各个类别之间的距离，得D(n+1)

  4. 跳至第2步，重复计算及合并

  5. 取距离阈值T，当D(n)的最小分量超过给定值T时，算法停止，所得即为聚类结果；

     若不取阈值，则直到全部样本聚为一类为止，输出聚类的分级树（得到全部可能的聚类结果——谱系聚类）

- 类间距离计算方法

  - 最短距离法：算出分别属于两个聚类中两个样本的欧氏距离，比较所有的欧氏距离，以最小的距离作为类间距离

  - 最长距离法：类比上

  - 中间距离法：若K类由I类和J类合并而成，则H和K类之间的距离为：

    $D_{HK}=\sqrt{\frac{1}{2}D^2_{HI}+\frac{1}{2}D^2_{HJ}-\frac{1}{4}D^2_{IJ}}$

  - 重心法

    $D_{HK}=\sqrt{\frac{n_I}{n_I+n_J}D^2_{HI}+\frac{n_J}{n_I+n_J}D^2_{HJ}-\frac{n_In_J}{(n_I+n_J)^2}D^2_{IJ}}$

  - 类平均距离法

    $D_{HK}=\sqrt{\frac{1}{n_Hn_K}\sum_{i\in H,j\in K}d^2_{ij}}$

    $d^2_{ij}$：H类任一样本Xi和K类任一样本Xj之间的欧氏距离平方



## 2.5 动态聚类法

### K-均值算法

- 算法描述

  1. 任选K个初始聚类中心：$Z_1(1),Z_2(1),...,Z_K(1)$（括号内为迭代的次序号）

  2. 按最小距离原则将其余样本分配到K个聚类中心中的一个

  3. 再次计算各个聚类中心的新向量值$Z_j(k+1)$

  4. 如果$Z_j(k+1) \ne Z_j(k)$，则回到2.，将模式样本逐个重新分类，重复迭代计算。

     如果$Z_j(k+1) = Z_j(k)$，算法收敛，计算完成。

- 根据$J_j-K$关系曲线，曲线拐点对应着接近最优的K值





# 3 判别函数分类法

## 3.1 判别函数

- 统计模式识别：按**任务类型**划分：
  - 聚类分析
  - 判别分析
    - 几何分类法
    - 概率分类法
    - 近邻分类法
- 判别函数定义：直接用来对模式进行分类的决策函数
- 判别函数正负值的确定：是在训练判别函数的权值时**人为确定的**

## 3.2 线性判别函数

- 一般形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}=W^T_0X+w_{n+1}$

  式中：$X=[x_1,x_2,...,x_n]^T$，$W_0=[w_1,w_2,...,w_n]^T$：权向量，即参数向量。

- 增广向量形式：

  $d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}·1=[w_1 w_2 ... w_n w_{n+1}]\left[\begin{matrix} x_1 \\ x_2 \\...\\x_n\\1 \end{matrix}\right]=W^TX$

### 3.2.2 线性判别函数的性质

1. 两类情况

$$
d(X)=W^TX \left\{
\begin{aligned}
>0,若X\in w_1 \\
<0,若X\in w_2
\end{aligned}
\right.
$$

2. 多类情况

   - 对M个线性可分模式类，$w_1,w_2,...,w_M$，有三种分类方式：

     - $w_i/\overline{w_i}$，是非两分法
     - $w_i/w_j$，成对两分法
     - $w_i/w_j$，成对两分法特例（没有不确定区）

   - **多类情况一：是非两分法**
     $$
     d_i(X)=W^T_iX\left\{
     \begin{aligned}
     >0, 若X\in w_i \\
     <0, 若X\in \overline{w_i}
     \end{aligned}
     \right.   i=1,..., M
     $$

     - 将某个待分类模式X 分别代入M 个类的d (X)中，
       **若只有$d_i(X)>0$，其他$d_j(X)$均<0，则判为$w_i$类**
     - ![3](模式识别.assets/3.PNG)

   - **多类情况二：成对两分法**

     - 一个判别界面只能分开两个类别，不需要把其余所有的类别都分开。

     - 判决函数：$d_{ij}(X)=W^T_{ij}X$，这里$d_{ji}=-d_{ij}$

       $d_{ij}(X)>0, \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

     - 在M类模式中，**以i开始的**M-1个判决函数全为正时，$X\in w_i$，其中若有一个为负，则为IR区

     - 能用本方法分类的模式集称为：**成对线性可分的**

     - M类共需要$C^2_M=\frac{M(M-1)}{2!}$ 个判决函数

   - **多类情况三：成对两分法特例（没有不确定区）**

     - 若成对两分法中的判别函数，可以分解为
       $$
       d_{ij}(X)=d_i(X)-d_j(X)
       $$
       时，那么$d_i(X)>d_j(X)$就相当于成对两分法中的$d_{ij}(X)>0$

       此时，对M类情况，等价的判别函数性质为：

       $d_i(X)>d_j(X), \forall j\ne i; i,j =1,2,...,M, 若X\in w_i$

     - 能用本方法分类的模式集称为：**线性可分的**

     - 若在第三种情况下可分，则在第二种情况下也可分，但反之不一定成立

     - 除边界区外，**没有不确定区域**

   - $w_i/\overline{w_i}（是非两分法）与w_i/w_j（成对两分法）$两分法的比较：

     - $w_i/\overline{w_i}$两分法共需要**M**个判别函数，$w_i/w_j$需要M(M-1)/2个
     - 当M>3时，后者需要更多的判别式，但对模式集进行线性可分的可能性要更大一些
       - 一种类别模式$w_j$的分布要比M-1类模式$\overline{w_i}$的分布更为聚集，因
         此$w_i/w_j$两分法受到的限制比$w_i/\overline{w_i}$少，因此线性可分可能性大。

## 3.3 广义线性判别函数

- 广义线性判别的目的：

  - 通过某映射，把模式空间X变成X\*，以便将X空间中非线性可分的模式集变成在X\*空间中的线性可分的模式集

- 非线性多项式判别函数

  - 设一训练用模式集，{X}在模式空间X中线性不可分，非线性多项式判别函数形式如下：

    $d(X)=w_1f_1(X)+w_2f_2(X)+...+w_kf_k(X)+w_{k+1}=\sum^{k+1}_{i=1}w_if_i(X)$

    - $f_i(X)$取什么形式以及d(X)取多少项，取决于非线性边界的复杂程度

- 广义线性函数的模式向量定义为：

  $X^*=[x_1^*,x_2^*,...,x_k^*,1]^T=[f_1(X),f_2(X),...,f_k(X),1]^T$

- 存在的问题：

  - 非线性变换可能非常复杂
  - 维数大大增加，导致维数灾难

## 3.4 线性判别函数的几何性质

### 3.4.1 模式空间与超平面

概念：

- **模式空间**：以n维模式向量X的n个分量为坐标变量的欧氏空间。

- **模式向量**：点、有向线段。

- **线性分类**：用d(X)进行分类，相当于用超平面d(X)=0把模式空
  间分成不同的决策区域。

- **超平面的法向量**：

  - X1,X2在超平面上：

  ![4](模式识别.assets/4.PNG)

  ​	$W_0^T(X_1-X_2)=0$，W0即超平面d(X)的法向量

  ​	**超平面的单位法线向量为U：**

  ​	$U=\frac{W_0}{||W_0||}$，$||W_0||=\sqrt{w_1^2+w_2^2+...+w_n^2}$

  - X不在超平面上：

    ![5](模式识别.assets/5.PNG)

    **判别函数d(X)正比于点X到超平面的代数距离**

    $r=\frac{d(X)}{||W_0||}$

  - X在原点

    $r_0=\frac{w_{n+1}}{||W_0||}$

    **原点在超平面的正负侧位置由阈值权$w_{n+1}$决定**

### 3.4.2 权空间与权向量解

- 概念：

  - **权空间**：以$d(X)=w_1x_1+w_2x_2+...+w_nx_n+w_{n+1}$的权系数为坐标变量的（n+1）维欧氏空间，X为已知。
  - **增广权向量**：点、有向线段

- 线性分类

  设增广样本向量：
  $$
  w_1类：X_{11},X_{12},...,X_{1p} \\
  w_2类：X_{21},X_{22},...,X_{2q}
  $$
  使d(X)将w1和w2分开，需满足：
  $$
  d(X_{1i})>0,i=1,2,...,p \\
  d(X_{2i})<0,i=1,2,...,q
  $$
  g给w2的q个增广模式乘-1，统一为：
  $$
  d(X)>0,其中X=\left\{
  \begin{aligned}
  X_{1i},i=1,2,...,p \\
  -X_{2i},i=1,2,...,q
  \end{aligned}
  \right.
  $$
  这就是**样本的符号规范化**，X：**规范化增广样本向量**



### 3.4.3 线性二分能力

- 是指线性函数对给定的N个n维二类模式的全部可能的类别分布情况，能正确分类
  的情况数。

- 若限定用线性判别函数，且样本在模式空间是**良好分布的**，即在n维模式空间中没有(n+1)个模式位于(n－1)维子空间中，可以证明：N个n维二类模式集用线性判别函数能正确分类的方法总
  数，称为：该模式集的**线性二分总数：**
  $$
  D(N,n)=
  \left\{
  \begin{aligned}
  & 2\sum^n_{j=0}C^j_{N-1}=2(C^0_{N-1}+C^1_{N-1}+...+C^n_{N-1})， & N>n+1 \\
  & 2^N=2(C^0_{N-1}+C^1_{N-1}+...+C^{N-1}_{N-1}), & N\le n+1
  \end{aligned}
  \right.
  $$
  或该模式集的**线性二分概率**：
  $$
  P(N,n)=\frac{D(N,n)}{2^N}=
  \left\{
  \begin{aligned}
  & 2^{1-N}\sum^n_{j=0}C^j_{N-1}, &N>n+1 \\
  & 1, & N\le n+1
  \end{aligned}
  \right.
  $$









- **只要模式集中模式个数N小于或等于增广模式的维数（n+1），模式类总是线性可分的。**

- 若定义$\lambda = N/(n+1)$，可观察到：

  - 在$\lambda \le 1$时，也即在$N\le (n+1)$时，有P(N,n)=1.

    即：这样的N个模式是线性可分的。

  - 在$1\le \lambda \le 2$时，也即$(n+1)\le N\le 2(n+1)$时，$0.5\le P(N,n)\le 1$

    即：当N固定、n增加时，模式集的线性二分概率是增加的。

  - 在$\lambda >2$时，也即在$N\ge (n+1)$时，$P(N,n)<0.5$

    即：当n固定、N增加时，模式集的线性二分概率迅速下降。

  将$\lambda=2$时的N值定义为阈值$N_0$，称为**二分法能力（容量）**，即：
  $$
  N_0=2(n+1)
  $$









## 3.6 感知器算法

- 用以利用已知类别的模式样本训练权向量W。

#### 感知器算法：

$$
两类线性可分的模式类：\omega _1,\omega _2 ,设d(X)=W^TX, \\
其中，W=[w_1,w_2,w_3,...,w_4]^T,X=[x_1,x_2,...,x_n,1]^T \\
应具有性质d(X)=W^TX
\left\{
\begin{aligned}
& >0, &若X\in \omega_1 \\
& <0， &若X\in \omega_2
\end{aligned}
\right.
$$


- 规范化处理：即$\omega_2$类样本全部乘以-1，则有：
  $$
  d(X)=W^TX>0
  $$







- 感知器算法的基本思想：用训练模式验证当前权向量的合理性，如果不合理，就根据误差进行反向纠正，直到全部训练样本都被合理分类。本质上是**梯度下降方法类**。

- 算法步骤：

  - 选择N个分属于$w_1$和$w_2$类的模式样本构成训练样本集$\{X_1,...,X_N\}$

    构成增广向量形式，并进行规范化处理。任取权向量初始值W(1)，开始迭代。初始迭代次数k=1

  - 用全部训练样本进行一轮迭代，计算$W^T(k)X_i$的值，并修正权向量。

    - 若$W^T(k)X_i\le 0$，说明分类器对第i个模式做了错误分类，校正为：
      $$
      W(k+1)=W(k)+cX_i
      $$

      - c：正的校正增量（步长）

    - 若$W^T(k)X_i>0$，分类正确，权向量不变：$W(k+1)=W(k)$

      统一写为：
      $$
      W(k+1)=
      \left\{
      \begin{aligned}
      &W(k),&若W^T(k)X_i>0 \\
      &W(k)+cX_i, &若W^T(k)X_i\le 0
      \end{aligned}
      \right.
      $$

  - 只要有一个错误分类，回到上一步，直到对所有样本正确分类。

- **感知器算法是一种赏罚过程。**

- 只要模式类别是线性可分的，就可以在有限的迭代步数里求出权向量的解。

#### 感知器算法用于多类情况

- 对于多类情况1和2，每个判别函数相当于一个两类问题的判别函数，因此可以用感知器算法直接学习判别函数的权向量。

- 对多类情况3，应有：

  > 若$X\in \omega_i，则d_i(X)>d_j(X), \forall j\ne i, j=1,...,M$， 对于M类模式应存在M个判决函数：{$d_i,i=1,...,M$}

- 算法主要内容：

  - 训练样本为增广向量模式，但**不需要符号规范化处理**。

  - 第k次迭代时，计算所有判别函数
    $$
    d_j(k)=W^T_j(k)X,j=1,...,M
    $$
    分两种情况修改权向量：

    1. 若$d_i(k)>d_j(k)，\forall j\ne i,j=1,2,...,M$，则权向量不变，$W_j(k+1)=W_j(k),j=1,2,...,M$

    2. 若第l个权向量使$d_i(k)\le d_l(k)$，则相应的权向量做调整：
       $$
       \left\{
       \begin{aligned}
       &W_i(k+1)=W_i(k)+cX \\
       &W_l(k+1)=W_l(k)-cX \\
       &W_j(k+1)=W_j(k),j\ne i,l
       \end{aligned}
       \right.
       ,c为正的校正增量
       $$









## 3.7 梯度算法

### 3.7.1 梯度算法基本原理

#### 梯度概念

设函数f(Y)是向量$Y=[y_1,y_2,...,y_n]^T$的函数，则f(Y)的梯度定义为：
$$
\nabla f(Y)=\frac{d}{dY}f(Y)=[\frac{\partial f}{\partial y_1},\frac{\partial f}{\partial y_2},...,\frac{\partial f}{\partial y_n}]^T
$$

- 重要性质：**指出函数f在其自变量增加时，增长最快的方向**
  - 即：梯度的**方向**是函数$f(Y)$在Y点增长最快的方向
  - 梯度的**模**是$f(Y)$在增长最快的方向上的增长率（增长率最大值）
  - 所以**负梯度指出了最陡下降方向**

#### 梯度算法

设两个线性可分的模式类$w_i$和$w_2$的样本共N个，$w_2$类样本乘（-1），将两类样本分开的判决函数d(X)应满足：
$$
d(X_i)=W^TX_i>0, i=1,2,...,N
$$
思想：**将联立不等式求解W的问题，转换成求准则函数极小值的问题。**

##### 基本思路

定义一个对错误分类敏感的准则函数$J(W,X)$，在J的梯度方向上对权向量进行修改，一般关系表示成从$W(k)$导出$W(k+1)$：
$$
W(k+1)=W(k)+c(-\nabla J)=W(k)-c\nabla J \\
W(k+1)=W(k)-c[\frac{\partial J(W,x)}{\partial W}]_{W=W(k)}
$$
其中c是正的比例因子。

##### 求解步骤

1. 规范化处理，选择准则函数，设置初始权向量W(1)，括号内为迭代次数k=1

2. 依次输入训练样本X，设第k次迭代时输入样本为$X_i$，此时以有权向量$W(k)$，求$\nabla J(k)$：
   $$
   \nabla J(k)=\frac{\partial J(W,X_i)}{\partial W}|_{W=W(k)}
   $$
   权向量修正为：
   $$
   W(k+1)=W(k)-c\nabla J(k)
   $$
   迭代次数加1，重复该步骤，直至对全部训练样本完成一轮迭代。

3. 在一轮迭代中，如果有一个样本使$\nabla J\ne 0$，回到2. 进行下一轮迭代，否则，W不再变化，算法收敛。

### 3.7.2 固定增量法

准则函数：**$J(W,X)=\frac{1}{2}(|W^TX|-W^TX)$**

当$W^TX>0$时，该准则函数有唯一最小值0

**固定增量算法：**
$$
\nabla J=\frac{\partial J(W,X)}{\part W}=\frac{1}{2}[Xsgn(W^TX)-X], \\
其中，sgn(W^TX)=\left\{
\begin{aligned}
& +1, &若W^TX>0 \\
&-1, &若W^TX\le 0
\end{aligned}
\right.
$$

$$
W(k+1)=W(k)+\left\{
\begin{aligned}
& 0,&若W^T(k)X>0 \\
& cX, &若W^T(k)X\le 0
\end{aligned}
\right.
$$

**感知器算法是梯度法的特例。**



## 3.8 最小平方误差算法（最小均方误差算法）

- LMSE算法，亦称为Ho-Kashyap算法
- 感知器算法、梯度算法、固定增量算法或其他类似方法，只有当模式类可分离时才收敛，不可分情况下，算法会来回摆动，始终不收敛。
- LMSE算法特点：
  - 对可分模式收敛
  - 对类别不可分的情况也能指出来

### 分类器的不等式方程

如果给出分属于$\omega_1,\omega_2$两个模式类的训练样本集$\{X_i,i=1,2,...,N\}$，应满足：$W^TX_i>0$，其中，$X_i$是规范化增广样本向量，$X_i=[x_{i1},x_{i2},...,x_{in},1]^T$

写成矩阵：
$$
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1n} & 1 \\
x_{21} & x_{22} & \cdots & x_{2n} & 1 \\
\vdots & \vdots & \vdots & \vdots & \vdots \\
-x_{N1} & -x_{N2} & ... & x_{Nn} & 1 \\
\end{bmatrix}_{N*(n+1)}
\begin{bmatrix}
w_1 \\
w_2 \\
\vdots \\
w_n \\
w_{n+1}
\end{bmatrix}_{(n+1)*1}>
\begin{bmatrix}
0 \\
0 \\
\vdots \\
0 \\
0 \\
\end{bmatrix}_{N*1}=0
$$
即$XW>0$

### LMS算法

- 原理：把对满足$XW>0$的求解，改为满足$XW=B$的求解。

  ​	   其中B为各分量均为正值的矢量。

  - 因为一般训练样本数N总是大于模式的维数n，所以为矛盾方程组，无解，只能求近似解，即
    $$
    ||XW^*-B||=极小
    $$

  - **LMS出发点**：选择一个准则函数，使得当J达到最小值时，XW=B可得到近似解（**最小二乘近似解**）

- 准则函数定义为
  $$
  J(W,X,B)=\frac{1}{2}||XW-B||^2
  $$

  - 最小二乘：
    - 最小：使方程组两边误差最小，即J最小
    - 二乘：二次方
    - 即**最小平方（误差算法）**

- LMS算法核心：通过准则函数极小找W、B

- 算法递推公式：
  $$
  B(k+1)=B(k)+c[e(k)+|e(k)|] \\
  W(k+1)=W(k)+cX^\#|e(k)|\quad其中，X^\#=(X^TX)^{-1}X^T
  $$






#### 模式类别可分性判别

- **当模式集线性可分，且校正系数c满足$0<c\le 1$时，该算法收敛，可求得解W。**

-  通常每次迭代计算后都检查一下**XW(k)**和误差向量**e(k)**，从而可以判断是否收敛：

  分以下几种情况：
$$
\begin{aligned}
  &①\ 如果e(k)=0，表明XW(k)=B(k)>0，有解。 \\
  &②\ 如果e(k)>0，表明XW(k)>B(k)>0，隐含有解，继续迭代，
  可使e(k)\rightarrow0.\\
  &③\ 如果e(k)<0（所有分量为负数或零，但不全为零），停止迭代，无解。
  \end{aligned}
$$
  综上所述：**只有当e(k)中有大于零的分量（说明：含有纠正信息）时，才需要继续迭代。**一旦e(k)的全部分量只有0和负数，则立即停止。

#### 算法描述

1. 根据N个分属于两类的样本，写出规范化增广样本矩阵X
2. 求X的伪逆矩阵$X^\#=(X^TX)^{-1}X^T$
3. 设置初值c和B(1)，c为正的校正增量，B(1)的各分量大于零，迭代次数k=1.开始迭代
4. 计算e(k)=XW(k)-B(k)，进行可分性判别。
5. 计算W(k+1)和B(k+1)，迭代次数加1，返回4.



## 3.9 非线性判别函数

- 用于线性不可分情况。如分段线性、超曲面。

### 3.9.1 分段线性判别函数

#### 一般分段线性判别函数

设有M类模式，将$\omega_i$类划分为$l_i$个子类：

$\omega_i:\{\omega_i^1,\omega_i^2,\omega_i^3,...,\omega_i^{l_i}\}\quad i=1,2,...,M$

其中第n个子类的判别函数：

$d_i^n(X)=(W_i^n)^TX \quad n=1,2,...,l_i;\ i=1,2,...,M$

$\omega_i$类的判别函数定义为：

$d_i(X)=max\{d_i^n(X),\quad n=1,2,...,l_i\}$

M类的判决规则：

$若d_j(X)=max\{d_i(X),\quad i=1,2,...,M\}，则X\in \omega_j$

#### 基于距离的分段线性判别函数

##### 最小距离分类器

$$
\begin{split}

&设\omega_1类均值向量：M_1=\frac{1}{N_1}\sum^{N_1}_{i=1}X_i \\
&w_2类均值向量：M_2=\frac{1}{N_2}\sum^{N_2}_{i=1}X_i
\\ \\ 
&任一模式X到M_1和M_2的欧氏距离平方：\\
&d_1(X)=||X-M_1||^2 \quad d_2(X)=||X-M_2||^2  \\
\\
&判决规则：若d_1(X)<d_2(X),则X\in \omega_1 \\
&若d_2(X)<d_1(X)，则X\in \omega_2 \\
&判别界面方程： \\
&||X-M_1||^2=||X-M_2||^2 \\
&化简得：2(M_1-M_2)^TX+(M_2^TM_2-M_1^TM_1)=0
\end{split}
$$

##### 分段线性距离分类器

$$
\begin{split}
& 设：M类模式，其中\omega_i类划分为l_i个子类，第n个子类的均值向量为M_i^n。每个子类的判别函数：\\
&\quad \quad d_i^n(X)=||X-M_i^n||^2,n=1,2,...,l_i;i=1,2,...,M \\
&每个类的判别函数：\\
&\quad\quad d_i(X)=min\{d_i^n(X),n=1,2,...,l_i\},i=1,2,...,M \\
&判决规则：\\
&\quad\quad若d_j(X)=min\{d_i(X),i=1,2,...,M\}，则X\in \omega_j
\end{split}
$$

### 3.9.2 分段线性判别函数的学习方法

1. 已知子类划分时的学习方法：
   - 每个子类看成独立的类
   - 在一类范围内根据多累情况3，学习各子类判别函数
   - 继而得到各类判别函数
2. 已知子类数目时的学习方法
   - 用类似于固定增量算法的错误修正算法学习分段线性判别函数。
3. 未知子类数目时的学习方法
   - 树状分段线性分类器

### 3.9.3 势函数法

#### 概念

- 每个点比拟为点能源，在点上势能达到峰值，随着与该点距离的增大，势能分布迅速减小。

  - $\omega_1$类样本势能为正——势能积累形成“高地”

  - $\omega_2$类样本势能*(-1)——势能积累形成“凹地”

    在两类电势分布之间，选择合适的等势面（如零等势面），即可认为是判别界面。

#### 势函数法判别函数的产生

设初始基类势函数$K_0(X)=0$，下标为迭代次数。

步骤：

1. 加入训练样本X1,
   $$
   K_1(X)=
   \left\{
   \begin{align}
   K_0(X)+K(X,X_1),若X_1\in \omega_1 \\
   K_0(X)-K(X,X_1),若X_1\in \omega_2
   \end{align}
   \right.
   $$
   $K_1(X)$描述了加入第一个样本后的边界划分。

2. 加第二个训练样本X2，分三种情况：
   $$
   \begin{align}
   (1)&若X_2\in\omega_1且K_1(X_2)>0,或X_2\in\omega_2且K_1(X_2)<0，\\
   &分类正确，势函数不变：K_2(X)=K_1(X) \\
   (2)&若X_2\in\omega_1但K_1(X_2)\le 0,错误分类，修改势函数：\\
   &K_2(X)=K_1(X)+K(X,X_2)=\pm K(X,X_1)+K(X,X_2) \\
   (3)&若X_2\in\omega_2但K_1(X_2)\ge0，错误分类，修改势函数： \\
   &K_2(X)=K_1(X)-K(X,X_2)=\pm K(X,X_1)-K(X,X_2)
   \end{align}
   $$
   .......

   以上决定积累位势的迭代算法可写成：
   $$
   K_{k+1}(X)=K_k(X)+r_{k+1}K(X,X_{k+1}) 
   $$
   其中$r_{k+1}$为校正项系数，定义为：
   $$
   r_{k+1}=
   \left\{
   \begin{align}
   0,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})>0 \\
   0,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})<0 \\
   1,\quad &对于X_{k+1}\in\omega_1且K_k(X_{k+1})\le0 \\
   -1,\quad &对于X_{k+1}\in\omega_2且K_k(X_{k+1})\ge0 \\
   \end{align}
   \right.
   $$
   略去不使积累势发生变化的样本，可得一简化样本序列$\{\widehat{X_1},\widehat{X_2},...,\widehat{X_j},...\}$，算法可归纳为：
   $$
   K_{k+1}(X)=\sum_{X_j}\alpha_jK(X,\widehat{X_j}),式中 \\
   \alpha_j=\left\{
   \begin{align}
   1,\quad &对于\widehat{X_j}\in\omega_1 \\
   -1,\quad&对于\widehat{X_j}\in\omega_2 
   \end{align}
   \right.
   $$
   即：由(k+1)个训练样本产生的积累势，等于两类中校正错误的样本的总势能之差。



#### 势函数的选择

- 势函数应具备的条件：

  两个n维向量$X$和$X_k$的函数$K(X,X_k)$，如同时满足下列三个条件，都可作为势函数：

  - $K(X,X_k)=K(X_k,X)$，当且仅当$X=X_k$时达到最大值
  - 当向量$X$与$X_k$的距离趋于无穷时，$K(X,X_k)$时趋于零
  - $K(X,X_k)$是光滑函数，且是$X$与$X_k$之间距离的单调下降函数

- 构成势函数的方法：

  - **Ⅰ型势函数**：用对称的有限项多项式展开，即：
    $$
    K(X,X_k)=\sum^m_{i=1}\varphi_i(X_k)\varphi_i(X)
    $$
    式中$\{ \varphi_i (X),i=1,2,...\}$，在模式定义域内应为正交函数集（即内积=0，内积：$(y,z)=\int^b_ay(x)z(x)d(x)$）

    判别函数：
    $$
    d_{k+1}(X)=d_k(X)+\sum^m_{i=1}r_{k+1}\varphi_i(X_{k+1})\varphi_i(X)
    $$

  - **Ⅱ型势函数**：直接选择双变量$X$和$X_k$对称函数作为势函数，即$K(X,X_k)=K(X_k,X)$ 

